{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from pathlib import Path\n",
    "import hu_core_ud_lg\n",
    "from collections import Counter\n",
    "nlp = hu_core_ud_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(Path.cwd() / 'Data' / 'WJ-comments' / 'comments.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(df):\n",
    "    res = {}\n",
    "    for topic in df:\n",
    "        comments = ' '.join([row for row in df.loc[df[topic].notnull(), topic]])\n",
    "        res[topic] = comments\n",
    "    return res\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Keeps only word chars and single white space from input text\n",
    "    '''\n",
    "    res = re.sub(r'[^\\w\\s]', '', text)\n",
    "    res = ' '.join([word.strip() for word in res.split()])\n",
    "    return res\n",
    "\n",
    "\n",
    "def tokenize_1gram(text, model, ents = []):\n",
    "    '''\n",
    "    Returns lowercase lemma for \n",
    "        -non-stop words, \n",
    "        -non-numbers, \n",
    "        -non-punct \n",
    "        and drops\n",
    "        -lemma of lenght 1\n",
    "        -entities defined by ents\n",
    "    '''\n",
    "    \n",
    "    doc = model(text)\n",
    "    res = []\n",
    "    \n",
    "    for word in doc:\n",
    "        if not word.is_stop and not word.is_punct and not word.like_num and len(word.lemma_) > 1 and word.ent_type_ not in ents:\n",
    "            res.append(word.lemma_.lower())\n",
    "            \n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_single(df, tokens_col):\n",
    "    '''\n",
    "    Removes words that only appear once in corpus \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    pd_series : pandas Series (df col)\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    Cleaned docs as list of lists\n",
    "    '''\n",
    "    all_tokens = sum(df[tokens_col], [])\n",
    "    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "    tokenized_no_single = [[word for word in text if word not in tokens_once] for text in df[tokens_col]]\n",
    "\n",
    "    return tokenized_no_single\n",
    "\n",
    "\n",
    "def get_word_counts(df, tokens_col):\n",
    "    res = {}\n",
    "    for index, row in df.iterrows():\n",
    "        wc = Counter(row[tokens_col])\n",
    "        res[index] = wc\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_comment = extract_comments(df)\n",
    "topic_comment = pd.DataFrame.from_dict(topic_comment, orient = 'index', columns = ['text'])\n",
    "topic_comment['tokenized'] = topic_comment['text']\\\n",
    "    .apply(lambda x: tokenize_1gram(clean(x), model = nlp, ents = ['PER', 'LOC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = get_word_counts(topic_comment, 'tokenized')\n",
    "topic_wc = pd.concat({k: pd.DataFrame.from_dict(v, 'index')\\\n",
    "                     for k, v in wc.items()}, axis = 0)\\\n",
    "                    .reset_index()\n",
    "topic_wc.columns = ['topic', 'word', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_wc.to_excel(Path.cwd() / 'Data' / 'WJ-comments' / 'topic_word_counts.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
